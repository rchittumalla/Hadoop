{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.4-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-61992ea324e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mwarehouse_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'hdfs:///apps/hive/warehouse/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Python Spark SQL Hive integration example\"\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.sql.warehouse.dir\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarehouse_location\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0menableHiveSupport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ramesh\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ramesh\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ramesh\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mc:\\users\\ramesh\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ramesh\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;31m# preexec_fn not supported on Windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                 \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ramesh\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[0;32m    707\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    710\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ramesh\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m    995\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 997\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m    998\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "## Create SparkContext, SparkSession\n",
    "from os.path import expanduser, join, abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = 'hdfs:///apps/hive/warehouse/'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://z.insofe.edu.in:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL Hive integration example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2f8bd50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify Spark Driver\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are 6 tables available in HIVE, in the database *insofe_empdb**\n",
    "\n",
    "Access these tables and process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify HIVE the Data in Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insofe_empdb\n",
    "spark.sql(\"use insofe_empdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify/List all the tables in the above database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+\n",
      "|    database|   tableName|isTemporary|\n",
      "+------------+------------+-----------+\n",
      "|insofe_empdb| departments|      false|\n",
      "|insofe_empdb|    dept_emp|      false|\n",
      "|insofe_empdb|dept_manager|      false|\n",
      "|insofe_empdb|   employees|      false|\n",
      "|insofe_empdb|    salaries|      false|\n",
      "|insofe_empdb|      titles|      false|\n",
      "+------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame for departments data from departments table in HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF = spark.sql(\"SELECT * FROM departments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the departments DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|dept_no|         dept_name|      last_modified|\n",
      "+-------+------------------+-------------------+\n",
      "|   d001|         Marketing|2013-01-28 23:59:59|\n",
      "|   d002|           Finance|2013-01-28 23:59:59|\n",
      "|   d003|   Human Resources|2013-01-28 23:59:59|\n",
      "|   d004|        Production|2013-01-28 23:59:59|\n",
      "|   d005|       Development|2013-01-28 23:59:59|\n",
      "|   d006|Quality Management|2013-01-28 23:59:59|\n",
      "|   d007|             Sales|2013-01-28 23:59:59|\n",
      "|   d008|          Research|2013-01-28 23:59:59|\n",
      "|   d009|  Customer Service|2013-01-28 23:59:59|\n",
      "|   d010|         Analytics|2018-07-05 09:02:09|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the Schema of deparaments DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- last_modified: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame for departments&employees from dept_emp table in HIVE\n",
    "<br>For each employee his/her respective department is available in dept_emp table\n",
    "<br>An employee might move/work in different times in his/her tenure\n",
    "<br>So, there will be multiple records for an employee (with different departments in different time periods)\n",
    "<br>At any time an employee must be active, only in one department and this can be identified with to_date of '9999-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_empDF = spark.sql(\"select * from dept_emp \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the departments&employees DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+----------+----------+-------------------+\n",
      "|seq_no|emp_no|dept_no| from_date|   to_date|      last_modified|\n",
      "+------+------+-------+----------+----------+-------------------+\n",
      "|     1|     1|   d001|1986-01-01|9999-01-01|2013-01-28 23:59:59|\n",
      "|     2|     2|   d002|1986-01-01|9999-01-01|2013-01-28 23:59:59|\n",
      "|     3|     3|   d003|1986-01-01|9999-01-01|2013-01-28 23:59:59|\n",
      "|     4|     4|   d004|1986-01-01|9999-01-01|2013-01-28 23:59:59|\n",
      "+------+------+-------+----------+----------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_empDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the schema for departments&employees DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seq_no: integer (nullable = true)\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date: string (nullable = true)\n",
      " |-- to_date: string (nullable = true)\n",
      " |-- last_modified: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_empDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Count in departments&employees DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341603"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_empDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter only the active records from the above DataFrame\n",
    "Active employees are whose to_date is '9999-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dept_empDF = dept_empDF[dept_empDF.to_date == '9999-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active employees count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250124"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_dept_empDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seq_no', 'int'),\n",
       " ('emp_no', 'int'),\n",
       " ('dept_no', 'string'),\n",
       " ('from_date', 'string'),\n",
       " ('to_date', 'string'),\n",
       " ('last_modified', 'timestamp')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_dept_empDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_no', 'int'),\n",
       " ('dept_no', 'string'),\n",
       " ('from_date', 'string'),\n",
       " ('to_date', 'string')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, column\n",
    "active_dept_empDF = active_dept_empDF.select('emp_no', 'dept_no', 'from_date', 'to_date')\n",
    "active_dept_empDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the DataFrame available in in-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_no: int, dept_no: string, from_date: string, to_date: string]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_dept_empDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame for departments&managers data from dept_manager table in HIVE\n",
    "<br>For each department respective manager's employee number is available in dept_manager table\n",
    "<br>A department may have multiple manager's \n",
    "<br>So, there will be multiple records for a department (with different employee number's for different time periods)\n",
    "<br>At any time an there will be only one active manager for each department and is can be identified by to_date value '9999-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_managerDF = spark.sql(\"select * from dept_manager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the departments&managers DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+-------------------+\n",
      "|seq_no|dept_no|emp_no| from_date|   to_date|      last_modified|\n",
      "+------+-------+------+----------+----------+-------------------+\n",
      "|     1|   d001|     1|1986-01-01|1992-10-01|2013-01-28 23:59:59|\n",
      "|     2|   d002|     2|1986-01-01|1990-12-17|2013-01-28 23:59:59|\n",
      "|     3|   d003|     3|1986-01-01|1993-03-21|2013-01-28 23:59:59|\n",
      "|     4|   d004|     4|1986-01-01|1989-09-09|2013-01-28 23:59:59|\n",
      "+------+-------+------+----------+----------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_managerDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the schema for departments&managers DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seq_no: integer (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- from_date: string (nullable = true)\n",
      " |-- to_date: string (nullable = true)\n",
      " |-- last_modified: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_managerDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Count in departments&managers DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_managerDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter only the active records from above DataFrame\n",
    "<br>Though there are only total 10 departments, but there are 25 records (manager records) exists, \n",
    "<br>remove the inactive records\n",
    "<br>Active records are whose to_date is '9999-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dept_managerDF = dept_managerDF[dept_managerDF.to_date=='9999-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the records count from the above DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_dept_managerDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify for the unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seq_no', 'int'),\n",
       " ('dept_no', 'string'),\n",
       " ('emp_no', 'int'),\n",
       " ('from_date', 'string'),\n",
       " ('to_date', 'string'),\n",
       " ('last_modified', 'timestamp')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_dept_managerDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted columns and rename the columns as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dept_managerDF = active_dept_managerDF.select('dept_no', expr('emp_no AS mgr_emp_no'), expr('from_date AS mgr_from_date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the above DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+\n",
      "|dept_no|mgr_emp_no|mgr_from_date|\n",
      "+-------+----------+-------------+\n",
      "|   d002|        10|   1990-12-17|\n",
      "|   d003|     19827|   1993-03-21|\n",
      "|   d001|     45502|   1993-10-01|\n",
      "|   d005|     64439|   1995-04-25|\n",
      "+-------+----------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "active_dept_managerDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame for employees data from employees table in HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.sql(\"select * from employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify employees DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+------------+------+----------+-------------------+\n",
      "|emp_no|birth_date|first_name|   last_name|gender| hire_date|      last_modified|\n",
      "+------+----------+----------+------------+------+----------+-------------------+\n",
      "|     1|1958-09-12| Margareta|  Markovitch|     M|1986-01-01|2013-01-28 23:59:59|\n",
      "|     2|1961-10-28|      Ebru|       Alpin|     M|1986-01-01|2013-01-28 23:59:59|\n",
      "|     3|1955-06-24|   Shirish|Ossenbruggen|     F|1986-01-01|2013-01-28 23:59:59|\n",
      "|     4|1958-06-08| Krassimir|     Wegerle|     F|1986-01-01|2013-01-28 23:59:59|\n",
      "+------+----------+----------+------------+------+----------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify schema of employees DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- last_modified: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = employeesDF.drop('last_modified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify above DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+------------+------+----------+\n",
      "|emp_no|birth_date|first_name|   last_name|gender| hire_date|\n",
      "+------+----------+----------+------------+------+----------+\n",
      "|     1|1958-09-12| Margareta|  Markovitch|     M|1986-01-01|\n",
      "|     2|1961-10-28|      Ebru|       Alpin|     M|1986-01-01|\n",
      "|     3|1955-06-24|   Shirish|Ossenbruggen|     F|1986-01-01|\n",
      "|     4|1958-06-08| Krassimir|     Wegerle|     F|1986-01-01|\n",
      "+------+----------+----------+------------+------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame for salaries data from salaries table in HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "salariesDF = spark.sql(\"select * from salaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify salaries DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+----------+----------+-------------------+\n",
      "|seq_no|emp_no|salary| from_date|   to_date|      last_modified|\n",
      "+------+------+------+----------+----------+-------------------+\n",
      "|     1|     1| 70166|1986-01-01|1987-01-01|2013-01-28 23:59:59|\n",
      "|     2|     1| 70820|1987-01-01|1988-01-01|2013-01-28 23:59:59|\n",
      "|     3|     1| 71970|1988-01-01|1989-01-01|2013-01-28 23:59:59|\n",
      "|     4|     1| 75211|1989-01-01|1989-12-31|2013-01-28 23:59:59|\n",
      "+------+------+------+----------+----------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salariesDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify schema of salaries DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seq_no: integer (nullable = true)\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- from_date: string (nullable = true)\n",
      " |-- to_date: string (nullable = true)\n",
      " |-- last_modified: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salariesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify count of records in salaries DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2854047"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salariesDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter only the active records from above DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_salariesDF = salariesDF[salariesDF.to_date=='9999-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the record count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250124"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_salariesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seq_no', 'int'),\n",
       " ('emp_no', 'int'),\n",
       " ('salary', 'int'),\n",
       " ('from_date', 'string'),\n",
       " ('to_date', 'string'),\n",
       " ('last_modified', 'timestamp')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_salariesDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove and rename unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, column\n",
    "active_salariesDF = active_salariesDF.select(\"emp_no\", \"salary\", expr(\"from_date as sal_from_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_no', 'int'), ('salary', 'int'), ('sal_from_date', 'string')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_salariesDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create titles DataFrame for the titles table in HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlesDF = spark.sql(\"select * from titles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify titles DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------+----------+----------+-------------------+\n",
      "|seq_no|emp_no|       title| from_date|   to_date|      last_modified|\n",
      "+------+------+------------+----------+----------+-------------------+\n",
      "|     1|     1|     Manager|1986-01-01|1992-10-01|2013-01-28 23:59:59|\n",
      "|     2|     1|Senior Staff|1992-10-01|9999-01-01|2013-01-28 23:59:59|\n",
      "|     3|     2|     Manager|1986-01-01|1990-12-17|2013-01-28 23:59:59|\n",
      "|     4|     2|Senior Staff|1990-12-17|9999-01-01|2013-01-28 23:59:59|\n",
      "+------+------+------------+----------+----------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titlesDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify titles DataFrame schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seq_no: integer (nullable = true)\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- from_date: string (nullable = true)\n",
      " |-- to_date: string (nullable = true)\n",
      " |-- last_modified: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titlesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify records count in the above DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453308"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlesDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter active records from the above DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_titlesDF = titlesDF[titlesDF.to_date=='9999-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seq_no', 'int'),\n",
       " ('emp_no', 'int'),\n",
       " ('title', 'string'),\n",
       " ('from_date', 'string'),\n",
       " ('to_date', 'string'),\n",
       " ('last_modified', 'timestamp')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_titlesDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove and rename the columns as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, column\n",
    "active_titlesDF = active_titlesDF.select('emp_no', 'title', expr('from_date AS title_from_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_no', 'int'), ('title', 'string'), ('title_from_date', 'string')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_titlesDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join department and departments_manager s DataFrames\n",
    "Result will have each department and corresponding manager's employee no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_curr_mgrDF = deptDF.join(active_dept_managerDF, 'dept_no', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+----------+-------------+\n",
      "|dept_no|         dept_name|      last_modified|mgr_emp_no|mgr_from_date|\n",
      "+-------+------------------+-------------------+----------+-------------+\n",
      "|   d002|           Finance|2013-01-28 23:59:59|        10|   1990-12-17|\n",
      "|   d003|   Human Resources|2013-01-28 23:59:59|     19827|   1993-03-21|\n",
      "|   d001|         Marketing|2013-01-28 23:59:59|     45502|   1993-10-01|\n",
      "|   d005|       Development|2013-01-28 23:59:59|     64439|   1995-04-25|\n",
      "|   d007|             Sales|2013-01-28 23:59:59|     71341|   1994-03-07|\n",
      "|   d008|          Research|2013-01-28 23:59:59|    107706|   1996-04-08|\n",
      "|   d006|Quality Management|2013-01-28 23:59:59|    149081|   2000-06-28|\n",
      "|   d009|  Customer Service|2013-01-28 23:59:59|    151543|   2003-01-03|\n",
      "|   d004|        Production|2013-01-28 23:59:59|    215054|   2005-08-30|\n",
      "|   d010|         Analytics|2018-07-05 09:02:09|    300030|   2013-01-29|\n",
      "+-------+------------------+-------------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_curr_mgrDF.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the manager's details by joining above DataFrame with employee's details using emp_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dept_no', 'string'),\n",
       " ('dept_name', 'string'),\n",
       " ('last_modified', 'timestamp'),\n",
       " ('mgr_emp_no', 'int'),\n",
       " ('mgr_from_date', 'string')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_curr_mgrDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_no', 'int'),\n",
       " ('birth_date', 'string'),\n",
       " ('first_name', 'string'),\n",
       " ('last_name', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('hire_date', 'string')]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_expr = dept_curr_mgrDF[\"mgr_emp_no\"] == employeesDF[\"emp_no\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_curr_mgr_detailsDF = dept_curr_mgrDF.join(employeesDF,join_expr,'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dept_no', 'string'),\n",
       " ('dept_name', 'string'),\n",
       " ('last_modified', 'timestamp'),\n",
       " ('mgr_emp_no', 'int'),\n",
       " ('mgr_from_date', 'string'),\n",
       " ('emp_no', 'int'),\n",
       " ('birth_date', 'string'),\n",
       " ('first_name', 'string'),\n",
       " ('last_name', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('hire_date', 'string')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_curr_mgr_detailsDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------------------+----------+-------------+------+----------+----------+----------+------+----------+\n",
      "|dept_no|      dept_name|      last_modified|mgr_emp_no|mgr_from_date|emp_no|birth_date|first_name| last_name|gender| hire_date|\n",
      "+-------+---------------+-------------------+----------+-------------+------+----------+----------+----------+------+----------+\n",
      "|   d002|        Finance|2013-01-28 23:59:59|        10|   1990-12-17|    10|1959-03-28|     Isamu|Legleitner|     F|1986-01-14|\n",
      "|   d003|Human Resources|2013-01-28 23:59:59|     19827|   1993-03-21| 19827|1960-12-02|   Karsten|   Sigstam|     F|1986-08-04|\n",
      "|   d001|      Marketing|2013-01-28 23:59:59|     45502|   1993-10-01| 45502|1967-06-21|  Vishwani|  Minakawa|     M|1988-04-12|\n",
      "|   d005|    Development|2013-01-28 23:59:59|     64439|   1995-04-25| 64439|1970-04-25|      Leon|  DasSarma|     F|1989-10-21|\n",
      "+-------+---------------+-------------------+----------+-------------+------+----------+----------+----------+------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_curr_mgr_detailsDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "replacements = {'birth_date' : 'mgr_birth_date', \n",
    "                'first_name' : 'mgr_first_name',\n",
    "                'last_name' : 'mgr_last_name',\n",
    "                'gender' : 'mgr_gender',\n",
    "                'hire_date' : 'mgr_hire_date'\n",
    "               }\n",
    "\n",
    "dept_curr_mgr_detailsDF = dept_curr_mgr_detailsDF.select([col(c).alias(replacements.get(c, c)) for c in dept_curr_mgr_detailsDF.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify above DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------------------+----------+-------------+------+--------------+--------------+-------------+----------+-------------+\n",
      "|dept_no|      dept_name|      last_modified|mgr_emp_no|mgr_from_date|emp_no|mgr_birth_date|mgr_first_name|mgr_last_name|mgr_gender|mgr_hire_date|\n",
      "+-------+---------------+-------------------+----------+-------------+------+--------------+--------------+-------------+----------+-------------+\n",
      "|   d002|        Finance|2013-01-28 23:59:59|        10|   1990-12-17|    10|    1959-03-28|         Isamu|   Legleitner|         F|   1986-01-14|\n",
      "|   d003|Human Resources|2013-01-28 23:59:59|     19827|   1993-03-21| 19827|    1960-12-02|       Karsten|      Sigstam|         F|   1986-08-04|\n",
      "|   d001|      Marketing|2013-01-28 23:59:59|     45502|   1993-10-01| 45502|    1967-06-21|      Vishwani|     Minakawa|         M|   1988-04-12|\n",
      "|   d005|    Development|2013-01-28 23:59:59|     64439|   1995-04-25| 64439|    1970-04-25|          Leon|     DasSarma|         F|   1989-10-21|\n",
      "+-------+---------------+-------------------+----------+-------------+------+--------------+--------------+-------------+----------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_curr_mgr_detailsDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_curr_mgr_detailsDF = dept_curr_mgr_detailsDF.drop('last_modified', 'emp_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dept_no', 'string'),\n",
       " ('dept_name', 'string'),\n",
       " ('mgr_emp_no', 'int'),\n",
       " ('mgr_from_date', 'string'),\n",
       " ('mgr_birth_date', 'string'),\n",
       " ('mgr_first_name', 'string'),\n",
       " ('mgr_last_name', 'string'),\n",
       " ('mgr_gender', 'string'),\n",
       " ('mgr_hire_date', 'string')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_curr_mgr_detailsDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+----------+-------------+--------------+--------------+-------------+----------+-------------+\n",
      "|dept_no|      dept_name|mgr_emp_no|mgr_from_date|mgr_birth_date|mgr_first_name|mgr_last_name|mgr_gender|mgr_hire_date|\n",
      "+-------+---------------+----------+-------------+--------------+--------------+-------------+----------+-------------+\n",
      "|   d002|        Finance|        10|   1990-12-17|    1959-03-28|         Isamu|   Legleitner|         F|   1986-01-14|\n",
      "|   d003|Human Resources|     19827|   1993-03-21|    1960-12-02|       Karsten|      Sigstam|         F|   1986-08-04|\n",
      "|   d001|      Marketing|     45502|   1993-10-01|    1967-06-21|      Vishwani|     Minakawa|         M|   1988-04-12|\n",
      "|   d005|    Development|     64439|   1995-04-25|    1970-04-25|          Leon|     DasSarma|         F|   1989-10-21|\n",
      "+-------+---------------+----------+-------------+--------------+--------------+-------------+----------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_curr_mgr_detailsDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join employees DataFrame with departments&employee DataFrame\n",
    "<br>Join employeesDF and active_dept_empDF based on emp_no\n",
    "<br>result DataFrame of the above is employee and his/her corresponding department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_no', 'int'),\n",
       " ('birth_date', 'string'),\n",
       " ('first_name', 'string'),\n",
       " ('last_name', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('hire_date', 'string')]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_no', 'int'),\n",
       " ('dept_no', 'string'),\n",
       " ('from_date', 'string'),\n",
       " ('to_date', 'string')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_dept_empDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_deptDF = active_dept_empDF.join(employeesDF,'emp_no','inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the above result DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+----------+----------+---------+------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+-------+----------+----------+----------+----------+---------+------+----------+\n",
      "|   148|   d005|1986-02-03|9999-01-01|1960-03-11|    Feipei| Nollmann|     M|1986-02-03|\n",
      "|   463|   d008|1986-02-06|9999-01-01|1955-04-15|Dharmaraja| Sadowsky|     M|1986-02-06|\n",
      "|   496|   d006|2001-09-02|9999-01-01|1964-03-29|      Mari|    Rotem|     M|1986-02-06|\n",
      "|   833|   d005|1994-03-25|9999-01-01|1961-09-14|      Huan|  Preusig|     M|1986-02-09|\n",
      "+------+-------+----------+----------+----------+----------+---------+------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_deptDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "replacements = {\n",
    "    'from_date' : 'dept_from_date',\n",
    "    'birth_date' : 'emp_birth_date',\n",
    "    'first_name' : 'emp_first_name',\n",
    "    'last_name' : 'emp_last_name',\n",
    "    'gender' : 'emp_gender',\n",
    "    'hire_date' : 'emp_hire_date'\n",
    "}\n",
    "\n",
    "emp_deptDF = emp_deptDF.select([col(c).alias(replacements.get(c, c)) for c in emp_deptDF.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------+----------+--------------+--------------+-------------+----------+-------------+\n",
      "|emp_no|dept_no|dept_from_date|   to_date|emp_birth_date|emp_first_name|emp_last_name|emp_gender|emp_hire_date|\n",
      "+------+-------+--------------+----------+--------------+--------------+-------------+----------+-------------+\n",
      "|     1|   d001|    1986-01-01|9999-01-01|    1958-09-12|     Margareta|   Markovitch|         M|   1986-01-01|\n",
      "|     2|   d002|    1986-01-01|9999-01-01|    1961-10-28|          Ebru|        Alpin|         M|   1986-01-01|\n",
      "|     3|   d003|    1986-01-01|9999-01-01|    1955-06-24|       Shirish| Ossenbruggen|         F|   1986-01-01|\n",
      "|     4|   d004|    1986-01-01|9999-01-01|    1958-06-08|     Krassimir|      Wegerle|         F|   1986-01-01|\n",
      "+------+-------+--------------+----------+--------------+--------------+-------------+----------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_deptDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify active records count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250124"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_deptDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a DataFrame with employees and respective manager's details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>DataFrame **emp_deptDF** contains all the active employees along with the their department\n",
    "<br>DataFrame **dept_curr_mgr_detailsDF** contains all the departments its manager''s details\n",
    "<br>Join these two DataFrames based on the dept_no, to result a DataFrame with employee''s along with the manager''s details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_no', 'int'),\n",
       " ('dept_no', 'string'),\n",
       " ('dept_from_date', 'string'),\n",
       " ('to_date', 'string'),\n",
       " ('emp_birth_date', 'string'),\n",
       " ('emp_first_name', 'string'),\n",
       " ('emp_last_name', 'string'),\n",
       " ('emp_gender', 'string'),\n",
       " ('emp_hire_date', 'string')]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_deptDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250124"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_deptDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dept_no', 'string'),\n",
       " ('dept_name', 'string'),\n",
       " ('mgr_emp_no', 'int'),\n",
       " ('mgr_from_date', 'string'),\n",
       " ('mgr_birth_date', 'string'),\n",
       " ('mgr_first_name', 'string'),\n",
       " ('mgr_last_name', 'string'),\n",
       " ('mgr_gender', 'string'),\n",
       " ('mgr_hire_date', 'string')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_curr_mgr_detailsDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_curr_mgr_detailsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join by broadcasting the smaller table - efficient join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "active_emp_dept_mgrDF = emp_deptDF.join(broadcast(dept_curr_mgr_detailsDF), 'dept_no', 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+----------+--------------+--------------+-------------+----------+-------------+---------------+----------+-------------+--------------+--------------+-------------+----------+-------------+\n",
      "|dept_no|emp_no|dept_from_date|   to_date|emp_birth_date|emp_first_name|emp_last_name|emp_gender|emp_hire_date|      dept_name|mgr_emp_no|mgr_from_date|mgr_birth_date|mgr_first_name|mgr_last_name|mgr_gender|mgr_hire_date|\n",
      "+-------+------+--------------+----------+--------------+--------------+-------------+----------+-------------+---------------+----------+-------------+--------------+--------------+-------------+----------+-------------+\n",
      "|   d001|     1|    1986-01-01|9999-01-01|    1958-09-12|     Margareta|   Markovitch|         M|   1986-01-01|      Marketing|     45502|   1993-10-01|    1967-06-21|      Vishwani|     Minakawa|         M|   1988-04-12|\n",
      "|   d002|     2|    1986-01-01|9999-01-01|    1961-10-28|          Ebru|        Alpin|         M|   1986-01-01|        Finance|        10|   1990-12-17|    1959-03-28|         Isamu|   Legleitner|         F|   1986-01-14|\n",
      "|   d003|     3|    1986-01-01|9999-01-01|    1955-06-24|       Shirish| Ossenbruggen|         F|   1986-01-01|Human Resources|     19827|   1993-03-21|    1960-12-02|       Karsten|      Sigstam|         F|   1986-08-04|\n",
      "|   d004|     4|    1986-01-01|9999-01-01|    1958-06-08|     Krassimir|      Wegerle|         F|   1986-01-01|     Production|    215054|   2005-08-30|    1981-07-27|         Oscar|     Ghazalie|         M|   2001-02-05|\n",
      "+-------+------+--------------+----------+--------------+--------------+-------------+----------+-------------+---------------+----------+-------------+--------------+--------------+-------------+----------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "active_emp_dept_mgrDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250124"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_emp_dept_mgrDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the DataFrame available in in-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dept_no: string, emp_no: int, dept_from_date: string, to_date: string, emp_birth_date: string, emp_first_name: string, emp_last_name: string, emp_gender: string, emp_hire_date: string, dept_name: string, mgr_emp_no: int, mgr_from_date: string, mgr_birth_date: string, mgr_first_name: string, mgr_last_name: string, mgr_gender: string, mgr_hire_date: string]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_emp_dept_mgrDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join Salaries and Titles DataFrames\n",
    "<br>**active_salariesDF** DataFrame contains the current salaries details of active employees\n",
    "<br>**active_titlesDF** DataFrame contains the current titles/designation details of active employees\n",
    "<br>join these two DataFrames based on the emp_no \n",
    "<br>result is DataFrame consists of all active employees along with their salaries and titles details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_no', 'int'), ('salary', 'int'), ('sal_from_date', 'string')]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_salariesDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_no', 'int'), ('title', 'string'), ('title_from_date', 'string')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_titlesDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_no: int, title: string, title_from_date: string]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_salariesDF.cache()\n",
    "active_titlesDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_sal_titlesDF = active_salariesDF.join(active_titlesDF, 'emp_no', 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+---------------+---------------+\n",
      "|emp_no|salary|sal_from_date|          title|title_from_date|\n",
      "+------+------+-------------+---------------+---------------+\n",
      "|   148|121640|   2003-01-30|Senior Engineer|     1993-02-04|\n",
      "|   463| 63130|   2003-02-02|   Senior Staff|     1986-02-06|\n",
      "|   496| 50281|   2002-08-16|       Engineer|     2000-08-17|\n",
      "|   833| 52747|   2003-03-23|Senior Engineer|     1994-03-25|\n",
      "+------+------+-------------+---------------+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_sal_titlesDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify record count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250124"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_sal_titlesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_no: int, salary: int, sal_from_date: string, title: string, title_from_date: string]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_sal_titlesDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Join \n",
    "<br>By now there are 2 DataFrames\n",
    "<br>**active_emp_dept_mgrDF** consists of all active employees along with the manager''s details.\n",
    "<br>**emp_sal_titlesDF** consists of the current salary and titles details for all the active employees.\n",
    "<br>join these two DataFrames to result a DataFrame with all the details for all active employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250124"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_emp_dept_mgrDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dept_no', 'string'),\n",
       " ('emp_no', 'int'),\n",
       " ('dept_from_date', 'string'),\n",
       " ('to_date', 'string'),\n",
       " ('emp_birth_date', 'string'),\n",
       " ('emp_first_name', 'string'),\n",
       " ('emp_last_name', 'string'),\n",
       " ('emp_gender', 'string'),\n",
       " ('emp_hire_date', 'string'),\n",
       " ('dept_name', 'string'),\n",
       " ('mgr_emp_no', 'int'),\n",
       " ('mgr_from_date', 'string'),\n",
       " ('mgr_birth_date', 'string'),\n",
       " ('mgr_first_name', 'string'),\n",
       " ('mgr_last_name', 'string'),\n",
       " ('mgr_gender', 'string'),\n",
       " ('mgr_hire_date', 'string')]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_emp_dept_mgrDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250124"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_sal_titlesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_no', 'int'),\n",
       " ('salary', 'int'),\n",
       " ('sal_from_date', 'string'),\n",
       " ('title', 'string'),\n",
       " ('title_from_date', 'string')]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_sal_titlesDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dept_no: string, emp_no: int, dept_from_date: string, to_date: string, emp_birth_date: string, emp_first_name: string, emp_last_name: string, emp_gender: string, emp_hire_date: string, dept_name: string, mgr_emp_no: int, mgr_from_date: string, mgr_birth_date: string, mgr_first_name: string, mgr_last_name: string, mgr_gender: string, mgr_hire_date: string]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_sal_titlesDF.cache()\n",
    "active_emp_dept_mgrDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_emp_detailsDF = active_emp_dept_mgrDF.join(emp_sal_titlesDF, 'emp_no', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_no: int, dept_no: string, dept_from_date: string, to_date: string, emp_birth_date: string, emp_first_name: string, emp_last_name: string, emp_gender: string, emp_hire_date: string, dept_name: string, mgr_emp_no: int, mgr_from_date: string, mgr_birth_date: string, mgr_first_name: string, mgr_last_name: string, mgr_gender: string, mgr_hire_date: string, salary: int, sal_from_date: string, title: string, title_from_date: string]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_emp_detailsDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------+----------+--------------+--------------+-------------+----------+-------------+------------------+----------+-------------+--------------+--------------+-------------+----------+-------------+------+-------------+---------------+---------------+\n",
      "|emp_no|dept_no|dept_from_date|   to_date|emp_birth_date|emp_first_name|emp_last_name|emp_gender|emp_hire_date|         dept_name|mgr_emp_no|mgr_from_date|mgr_birth_date|mgr_first_name|mgr_last_name|mgr_gender|mgr_hire_date|salary|sal_from_date|          title|title_from_date|\n",
      "+------+-------+--------------+----------+--------------+--------------+-------------+----------+-------------+------------------+----------+-------------+--------------+--------------+-------------+----------+-------------+------+-------------+---------------+---------------+\n",
      "|   148|   d005|    1986-02-03|9999-01-01|    1960-03-11|        Feipei|     Nollmann|         M|   1986-02-03|       Development|     64439|   1995-04-25|    1970-04-25|          Leon|     DasSarma|         F|   1989-10-21|121640|   2003-01-30|Senior Engineer|     1993-02-04|\n",
      "|   463|   d008|    1986-02-06|9999-01-01|    1955-04-15|    Dharmaraja|     Sadowsky|         M|   1986-02-06|          Research|    107706|   1996-04-08|    1962-06-27|        Hilary|       Kambil|         F|   1993-01-31| 63130|   2003-02-02|   Senior Staff|     1986-02-06|\n",
      "|   496|   d006|    2001-09-02|9999-01-01|    1964-03-29|          Mari|        Rotem|         M|   1986-02-06|Quality Management|    149081|   2000-06-28|    1972-08-19|          Dung|        Pesch|         M|   1995-06-09| 50281|   2002-08-16|       Engineer|     2000-08-17|\n",
      "|   833|   d005|    1994-03-25|9999-01-01|    1961-09-14|          Huan|      Preusig|         M|   1986-02-09|       Development|     64439|   1995-04-25|    1970-04-25|          Leon|     DasSarma|         F|   1989-10-21| 52747|   2003-03-23|Senior Engineer|     1994-03-25|\n",
      "+------+-------+--------------+----------+--------------+--------------+-------------+----------+-------------+------------------+----------+-------------+--------------+--------------+-------------+----------+-------------+------+-------------+---------------+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "active_emp_detailsDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derive additional columns such as\n",
    "-  emp_age = current_date - emp_birth_date\n",
    "-  emp_tenure = current_date - emp_hire_date\n",
    "-  mgr_age = current_date - mgr_birth_date\n",
    "-  mgr_tenure = current_date - mgr_hire_date\n",
    "-  salary_since = current_date - sal_from_date\n",
    "-  role_since = current_date - title_from_date\n",
    "-  emp_dept_tenure = current_date - dept_from_date\n",
    "-  mgr_dept_tenure = current_date - mgr_from_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a temporary table/view to perform sql queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_emp_detailsDF.registerTempTable(\"active_emp_details_sqlTBL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_no', 'int'),\n",
       " ('dept_no', 'string'),\n",
       " ('dept_from_date', 'string'),\n",
       " ('to_date', 'string'),\n",
       " ('emp_birth_date', 'string'),\n",
       " ('emp_first_name', 'string'),\n",
       " ('emp_last_name', 'string'),\n",
       " ('emp_gender', 'string'),\n",
       " ('emp_hire_date', 'string'),\n",
       " ('dept_name', 'string'),\n",
       " ('mgr_emp_no', 'int'),\n",
       " ('mgr_from_date', 'string'),\n",
       " ('mgr_birth_date', 'string'),\n",
       " ('mgr_first_name', 'string'),\n",
       " ('mgr_last_name', 'string'),\n",
       " ('mgr_gender', 'string'),\n",
       " ('mgr_hire_date', 'string'),\n",
       " ('salary', 'int'),\n",
       " ('sal_from_date', 'string'),\n",
       " ('title', 'string'),\n",
       " ('title_from_date', 'string')]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_emp_detailsDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the order of the columns with the select and derive the columns as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_employees_data  = spark.sql(\"\"\"\n",
    "SELECT emp_no, emp_first_name, emp_last_name, emp_gender, emp_birth_date, emp_hire_date,\n",
    "       round(datediff(current_date,to_date(emp_birth_date))/365) as emp_age,\n",
    "       round(datediff(current_date,to_date(emp_hire_date))/365) as emp_tenure,\n",
    "       salary, sal_from_date, \n",
    "       round(datediff(current_date,to_date(sal_from_date))/365) as salary_since,\n",
    "       title, title_from_date,\n",
    "       round(datediff(current_date,to_date(title_from_date))/365) as role_since,\n",
    "       dept_no, dept_name, dept_from_date,\n",
    "       round(datediff(current_date,to_date(dept_from_date))/365) as emp_dept_tenure,\n",
    "       mgr_emp_no, mgr_first_name, mgr_last_name, mgr_gender, mgr_birth_date, mgr_hire_date, mgr_from_date,\n",
    "       round(datediff(current_date,to_date(mgr_birth_date))/365) as mgr_age,\n",
    "       round(datediff(current_date,to_date(mgr_hire_date))/365) as mgr_tenure,\n",
    "       round(datediff(current_date,to_date(mgr_from_date))/365) as mgr_dept_tenure\n",
    "FROM active_emp_details_sqlTBL\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------+----------+--------------+-------------+-------+----------+------+-------------+------------+---------------+---------------+----------+-------+------------------+--------------+---------------+----------+--------------+-------------+----------+--------------+-------------+-------------+-------+----------+---------------+\n",
      "|emp_no|emp_first_name|emp_last_name|emp_gender|emp_birth_date|emp_hire_date|emp_age|emp_tenure|salary|sal_from_date|salary_since|          title|title_from_date|role_since|dept_no|         dept_name|dept_from_date|emp_dept_tenure|mgr_emp_no|mgr_first_name|mgr_last_name|mgr_gender|mgr_birth_date|mgr_hire_date|mgr_from_date|mgr_age|mgr_tenure|mgr_dept_tenure|\n",
      "+------+--------------+-------------+----------+--------------+-------------+-------+----------+------+-------------+------------+---------------+---------------+----------+-------+------------------+--------------+---------------+----------+--------------+-------------+----------+--------------+-------------+-------------+-------+----------+---------------+\n",
      "|   148|        Feipei|     Nollmann|         M|    1960-03-11|   1986-02-03|   58.0|      32.0|121640|   2003-01-30|        15.0|Senior Engineer|     1993-02-04|      25.0|   d005|       Development|    1986-02-03|           32.0|     64439|          Leon|     DasSarma|         F|    1970-04-25|   1989-10-21|   1995-04-25|   48.0|      29.0|           23.0|\n",
      "|   463|    Dharmaraja|     Sadowsky|         M|    1955-04-15|   1986-02-06|   63.0|      32.0| 63130|   2003-02-02|        15.0|   Senior Staff|     1986-02-06|      32.0|   d008|          Research|    1986-02-06|           32.0|    107706|        Hilary|       Kambil|         F|    1962-06-27|   1993-01-31|   1996-04-08|   56.0|      25.0|           22.0|\n",
      "|   496|          Mari|        Rotem|         M|    1964-03-29|   1986-02-06|   54.0|      32.0| 50281|   2002-08-16|        16.0|       Engineer|     2000-08-17|      18.0|   d006|Quality Management|    2001-09-02|           17.0|    149081|          Dung|        Pesch|         M|    1972-08-19|   1995-06-09|   2000-06-28|   46.0|      23.0|           18.0|\n",
      "|   833|          Huan|      Preusig|         M|    1961-09-14|   1986-02-09|   57.0|      32.0| 52747|   2003-03-23|        15.0|Senior Engineer|     1994-03-25|      24.0|   d005|       Development|    1994-03-25|           24.0|     64439|          Leon|     DasSarma|         F|    1970-04-25|   1989-10-21|   1995-04-25|   48.0|      29.0|           23.0|\n",
      "+------+--------------+-------------+----------+--------------+-------------+-------+----------+------+-------------+------------+---------------+---------------+----------+-------+------------------+--------------+---------------+----------+--------------+-------------+----------+--------------+-------------+-------------+-------+----------+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "active_employees_data.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the DataFrame to the persistent storage - HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_employees_data.repartition(1).write.option(\"header\", \"false\").csv(\"/user/manasm/datasets/employeesdb/results/active_employees_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_no: int, emp_first_name: string, emp_last_name: string, emp_gender: string, emp_birth_date: string, emp_hire_date: string, emp_age: double, emp_tenure: double, salary: int, sal_from_date: string, salary_since: double, title: string, title_from_date: string, role_since: double, dept_no: string, dept_name: string, dept_from_date: string, emp_dept_tenure: double, mgr_emp_no: int, mgr_first_name: string, mgr_last_name: string, mgr_gender: string, mgr_birth_date: string, mgr_hire_date: string, mgr_from_date: string, mgr_age: double, mgr_tenure: double, mgr_dept_tenure: double]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_employees_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250124"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_employees_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated Data\n",
    "#### Create the Aggregations\n",
    "-  Based on Department\n",
    "-  Based on Department and Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate based on department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "dept_aggrDF = active_employees_data.groupBy('dept_no').agg(\n",
    "    F.min('salary').alias('Min_Salary'),\n",
    "    F.max('salary').alias('Max_Salary'),\n",
    "    F.mean('salary').alias('Mean_Salary'),\n",
    "    F.count('salary').alias('Total_Employees'),\n",
    "    F.stddev('salary').alias('StdDev_Salary'),\n",
    "    F.sum('salary').alias('Total_salary'),\n",
    "    F.min('emp_age').alias('Min_Age'),\n",
    "    F.max('emp_age').alias('Max_Age'),\n",
    "    F.mean('emp_age').alias('Mean_Age'),\n",
    "    F.min('emp_tenure').alias('Min_Tenure'),\n",
    "    F.max('emp_tenure').alias('Max_Tenure'),\n",
    "    F.mean('emp_tenure').alias('Mean_Tenure'),\n",
    "    F.mean('salary_since').alias('Mean_Salary_Since'),\n",
    "    F.mean('role_since').alias('Mean_Role_Since')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dept_no: string, Min_Salary: int, Max_Salary: int, Mean_Salary: double, Total_Employees: bigint, StdDev_Salary: double, Total_salary: bigint, Min_Age: double, Max_Age: double, Mean_Age: double, Min_Tenure: double, Max_Tenure: double, Mean_Tenure: double, Mean_Salary_Since: double, Mean_Role_Since: double]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_aggrDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+------------------+---------------+------------------+------------+-------+-------+------------------+----------+----------+------------------+-----------------+------------------+\n",
      "|dept_no|Min_Salary|Max_Salary|       Mean_Salary|Total_Employees|     StdDev_Salary|Total_salary|Min_Age|Max_Age|          Mean_Age|Min_Tenure|Max_Tenure|       Mean_Tenure|Mean_Salary_Since|   Mean_Role_Since|\n",
      "+-------+----------+----------+------------------+---------------+------------------+------------+-------+-------+------------------+----------+----------+------------------+-----------------+------------------+\n",
      "|   d005|     12582|    142434|61388.935342615165|          62344|16393.484361748524|  3827231785|   24.0|   66.0| 46.82923777749262|       1.0|      33.0| 21.67711407673553| 9.85997048633389| 15.65260490183498|\n",
      "|   d009|     12505|    142950| 61567.09564047363|          18580|19054.432158113068|  1143916637|   24.0|   66.0| 46.40398277717976|       1.0|      33.0|20.944994617868677|9.601022604951561|14.990904198062433|\n",
      "|   d003|     12669|    140953|58621.627125972904|          13876|17636.231843258673|   813433698|   24.0|   66.0|46.205174401844914|       1.0|      33.0| 20.61912654943788|9.473335255116748|14.719227443067167|\n",
      "|   d001|     12501|    144128| 73716.20280959312|          15803|20121.259512783337|  1164937153|   24.0|   66.0| 46.25628045307853|       1.0|      33.0| 20.79155856482946| 9.53616401948997|14.839713978358539|\n",
      "|   d007|     12524|    157220|  82163.1371330952|          38634|19680.750681990237|  3174290640|   24.0|   66.0|46.722446549671275|       1.0|      33.0|  21.4983951959414| 9.78622456903246|15.331702645338304|\n",
      "|   d004|     12568|    137497| 61653.83507737656|          54280| 16490.60266483779|  3346570168|   24.0|   66.0|46.839812085482684|       1.0|      33.0|21.647752394988945|9.849871039056742|15.676271186440678|\n",
      "|   d010|     12567|    137498| 76304.74850299401|           1002| 36296.40611115705|    76457358|   24.0|   49.0| 35.73353293413174|       1.0|       5.0| 3.470059880239521|3.470059880239521| 3.470059880239521|\n",
      "|   d002|     12701|    141395| 72330.34274492368|          13494|20223.466108891476|   976025645|   24.0|   66.0| 46.15103008744627|       1.0|      33.0|20.518156217578184|9.442418852823478|14.669112198013933|\n",
      "|   d006|     12596|    137308|60143.686778493524|          15679|17906.764461726154|   942992865|   24.0|   66.0|46.198226927737736|       1.0|      33.0|20.586070540213022|9.472925569232732|14.970023598443778|\n",
      "|   d008|     12817|    137344|  62241.2610150925|          16432|18057.399737068674|  1022748401|   24.0|   66.0| 46.32850535540409|       1.0|      33.0| 20.83976387536514|9.559457156767284|14.953627069133399|\n",
      "+-------+----------+----------+------------------+---------------+------------------+------------+-------+-------+------------------+----------+----------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_aggrDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation based on Department and Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_gender_aggrDF = active_employees_data.groupBy('dept_no', 'emp_gender').agg(\n",
    "    F.min('salary').alias('Min_Salary'),\n",
    "    F.max('salary').alias('Max_Salary'),\n",
    "    F.mean('salary').alias('Mean_Salary'),\n",
    "    F.count('salary').alias('Total_Employees'),\n",
    "    F.stddev('salary').alias('StdDev_Salary'),\n",
    "    F.sum('salary').alias('Total_salary'),\n",
    "    F.min('emp_age').alias('Min_Age'),\n",
    "    F.max('emp_age').alias('Max_Age'),\n",
    "    F.mean('emp_age').alias('Mean_Age'),\n",
    "    F.min('emp_tenure').alias('Min_Tenure'),\n",
    "    F.max('emp_tenure').alias('Max_Tenure'),\n",
    "    F.mean('emp_tenure').alias('Mean_Tenure'),\n",
    "    F.mean('salary_since').alias('Mean_Salary_Since'),\n",
    "    F.mean('role_since').alias('Mean_Role_Since')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+------------------+---------------+------------------+------------+-------+-------+------------------+----------+----------+------------------+-----------------+------------------+\n",
      "|dept_no|emp_gender|Min_Salary|Max_Salary|       Mean_Salary|Total_Employees|     StdDev_Salary|Total_salary|Min_Age|Max_Age|          Mean_Age|Min_Tenure|Max_Tenure|       Mean_Tenure|Mean_Salary_Since|   Mean_Role_Since|\n",
      "+-------+----------+----------+----------+------------------+---------------+------------------+------------+-------+-------+------------------+----------+----------+------------------+-----------------+------------------+\n",
      "|   d006|         M|     12596|    137308| 59807.84364820847|           9210|17371.741914549362|   550830240|   24.0|   66.0|  46.3742671009772|       1.0|      32.0| 20.86243213897937| 9.57100977198697|15.130618892508144|\n",
      "|   d006|         F|     12776|    137294| 60621.83104034627|           6469|18632.929352115545|   392162625|   24.0|   66.0|  45.9475962281651|       1.0|      33.0| 20.19261091358788|9.333281805534085|14.741381975575823|\n",
      "|   d003|         M|     13320|    140953| 58390.39888322409|           8238|17382.666710193324|   481020106|   24.0|   66.0| 46.33624666181112|       1.0|      32.0|  20.8244719592134|9.544428259286235|14.866836610827871|\n",
      "|   d001|         F|     13110|    137842| 73401.52398232881|           6338| 20192.29637278024|   465218859|   24.0|   66.0|45.983117702745346|       1.0|      32.0|  20.4520353423793|9.403124013884506|14.568475859892711|\n",
      "|   d005|         F|     12582|    142434|61347.262718299164|          25023|16452.692992389355|  1535092555|   24.0|   66.0| 46.76997162610398|       1.0|      32.0|21.571594133397273|9.814490668584902|15.606721815929346|\n",
      "|   d009|         M|     12505|    142950|61337.880253050156|          11065|   18889.760887489|   678703645|   24.0|   66.0|46.452146407591506|       1.0|      32.0|21.093447808404882|9.647898779936737|15.061726163578852|\n",
      "|   d004|         F|     13612|    137497| 61600.81429550126|          21895| 16518.61693414313|  1348749829|   24.0|   66.0|46.814523863895865|       1.0|      33.0| 21.58305549212149|9.827403516784655|15.636629367435487|\n",
      "|   d009|         F|     12998|    140866| 61904.58975382568|           7515|  19290.6392600534|   465212992|   24.0|   66.0| 46.33306719893546|       1.0|      33.0| 20.72641383898869|9.532002661343979|14.886626746506986|\n",
      "|   d010|         F|     12567|    137498| 76325.34419551934|            491|36076.747302666736|    37475744|   24.0|   48.0| 36.20162932790224|       1.0|       5.0| 3.435845213849287|3.435845213849287| 3.435845213849287|\n",
      "|   d001|         M|     12501|    144128| 73926.91959852086|           9465|20071.861027664712|   699718294|   24.0|   66.0|  46.4391970417327|       1.0|      33.0|21.018911780243002|9.625250924458532|15.021341785525621|\n",
      "|   d003|         F|     12669|    137321|58959.487761617595|           5638| 17996.51950328604|   332413592|   24.0|   66.0|46.013657325292655|       1.0|      33.0| 20.31908478183753|9.369457254345512|14.503547357218872|\n",
      "|   d002|         M|     12701|    141395| 72112.88582924682|           7953| 20151.16751739652|   573513781|   24.0|   66.0| 46.22192883188734|       1.0|      33.0| 20.72060857538036|9.507229976109643|14.746385011945177|\n",
      "|   d007|         M|     12524|    157220| 82220.26261928408|          23159| 19551.94973561398|  1904139062|   24.0|   66.0|46.782978539660604|       1.0|      33.0|21.574420311757848|9.812513493674166| 15.37281402478518|\n",
      "|   d007|         F|     12632|    151687| 82077.64639741518|          15475|19872.274431122827|  1270151578|   24.0|   66.0| 46.63185783521809|       1.0|      32.0|21.384620355411954|9.746882067851374|15.270177705977384|\n",
      "|   d008|         F|     12904|    137284|  62530.6636677163|           6669|18237.142865819453|   417016996|   24.0|   65.0| 46.24351476983056|       1.0|      32.0|20.693207377417902|9.528714949767581|14.838656470235417|\n",
      "|   d002|         F|     13007|    137300| 72642.45876195632|           5541| 20324.54184952541|   402511864|   24.0|   66.0|46.049269085002706|       1.0|      32.0| 20.22757624977441|9.349395415989893|14.558202490525176|\n",
      "|   d008|         M|     12817|    137344|62043.573184471985|           9763| 17931.83570658838|   605731405|   24.0|   66.0|46.386561507733276|       1.0|      33.0|20.939875038410324|9.580456826795043|15.032162245211513|\n",
      "|   d004|         M|     12568|    137378| 61689.68161185734|          32385| 16471.79413468687|  1997820339|   24.0|   66.0| 46.85690906283774|       1.0|      32.0|21.691492975142815| 9.86506098502393|15.703072410066389|\n",
      "|   d005|         M|     12876|    137119|61416.876021542834|          37321|16353.827310278446|  2292139230|   24.0|   66.0|46.868974571956805|       1.0|      33.0|21.747863133356557|9.890463813938533|15.683368612845316|\n",
      "|   d010|         M|     12852|    137449| 76284.95890410959|            511|36541.558030365144|    38981614|   24.0|   49.0| 35.28375733855186|       1.0|       5.0|  3.50293542074364| 3.50293542074364|  3.50293542074364|\n",
      "+-------+----------+----------+----------+------------------+---------------+------------------+------------+-------+-------+------------------+----------+----------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_gender_aggrDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write above DataFrames to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_aggrDF.repartition(1).write.option(\"header\", \"false\").csv(\"/user/manasm/datasets/employeesdb/results/aggr_dept/\")\n",
    "dept_gender_aggrDF.repartition(1).write.option(\"header\", \"false\").csv(\"/user/manasm/datasets/employeesdb/results/aggr_dept_gender/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
